Starting train_sign.py at Sun Jun 22 03:21:01 CEST 2025
/work/courses/csnlp/Team3/envs/csnlp/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1750555280.570279 3078992 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5
I0000 00:00:1750555280.650951 3079044 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 570.86.15), renderer: NVIDIA GeForce GTX 1080 Ti/PCIe/SSE2
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
W0000 00:00:1750555280.777865 3079027 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
W0000 00:00:1750555280.847332 3079042 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
W0000 00:00:1750555280.853086 3079033 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
W0000 00:00:1750555280.855803 3079031 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
W0000 00:00:1750555280.863146 3079043 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
W0000 00:00:1750555280.895823 3079039 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
W0000 00:00:1750555280.900503 3079037 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
W0000 00:00:1750555280.909716 3079033 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
✅ WandB disabled for debugging
Loading SignVLMDynamic model...
Model loaded. Total parameters: 1,320,007,369
Trainable parameters: 84,192,969
Loading dataset...
Filtered out 6866 samples without frames
Limited dataset to 100 samples.
Training set size: 90
Validation set size: 10
Starting training...
Epoch 1:   0%|          | 0/23 [00:00<?, ?it/s]/work/courses/csnlp/Team3/slt/datasets/phoenix_simplified.py:180: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  motion = torch.load(motion_path, map_location="cpu")
/work/courses/csnlp/Team3/slt/datasets/phoenix_simplified.py:181: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  spatial = torch.load(spatial_path, map_location="cpu")
/work/courses/csnlp/Team3/slt/datasets/phoenix_simplified.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  landmarks = torch.load(landmark_path, map_location="cpu")
W0000 00:00:1750555312.675733 3079030 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
Epoch 1:   0%|          | 0/23 [00:34<?, ?it/s, loss=11.4508, lr=5.00e-07]Epoch 1:   4%|▍         | 1/23 [00:34<12:31, 34.15s/it, loss=11.4508, lr=5.00e-07]Epoch 1:   4%|▍         | 1/23 [00:36<12:31, 34.15s/it, loss=11.3942, lr=1.00e-06]Epoch 1:   9%|▊         | 2/23 [00:36<05:21, 15.29s/it, loss=11.3942, lr=1.00e-06]Epoch 1:   9%|▊         | 2/23 [00:38<05:21, 15.29s/it, loss=11.1396, lr=1.50e-06]Epoch 1:  13%|█▎        | 3/23 [00:38<03:02,  9.15s/it, loss=11.1396, lr=1.50e-06]Epoch 1:  13%|█▎        | 3/23 [00:40<03:02,  9.15s/it, loss=10.9094, lr=2.00e-06]Epoch 1:  17%|█▋        | 4/23 [00:40<02:02,  6.46s/it, loss=10.9094, lr=2.00e-06]Epoch 1:  17%|█▋        | 4/23 [00:41<02:02,  6.46s/it, loss=11.3062, lr=2.50e-06]Epoch 1:  22%|██▏       | 5/23 [00:41<01:24,  4.69s/it, loss=11.3062, lr=2.50e-06]Epoch 1:  22%|██▏       | 5/23 [00:43<01:24,  4.69s/it, loss=10.9670, lr=3.00e-06]Epoch 1:  26%|██▌       | 6/23 [00:43<01:04,  3.77s/it, loss=10.9670, lr=3.00e-06]Epoch 1:  26%|██▌       | 6/23 [00:46<01:04,  3.77s/it, loss=11.2444, lr=3.50e-06]Epoch 1:  30%|███       | 7/23 [00:46<00:51,  3.22s/it, loss=11.2444, lr=3.50e-06]Epoch 1:  30%|███       | 7/23 [00:47<00:51,  3.22s/it, loss=11.1740, lr=4.00e-06]Epoch 1:  35%|███▍      | 8/23 [00:47<00:41,  2.74s/it, loss=11.1740, lr=4.00e-06]Epoch 1:  35%|███▍      | 8/23 [00:50<00:41,  2.74s/it, loss=10.5897, lr=4.50e-06]Epoch 1:  39%|███▉      | 9/23 [00:50<00:37,  2.71s/it, loss=10.5897, lr=4.50e-06]Epoch 1:  39%|███▉      | 9/23 [00:53<00:37,  2.71s/it, loss=10.3960, lr=5.00e-06]Traceback (most recent call last):
  File "/work/courses/csnlp/Team3/slt/train_sign.py", line 372, in main
    wandb.log(log_dict)
  File "/work/courses/csnlp/Team3/envs/csnlp/lib/python3.10/site-packages/wandb/sdk/lib/preinit.py", line 36, in preinit_wrapper
    raise wandb.Error(f"You must call wandb.init() before {name}()")
wandb.errors.errors.Error: You must call wandb.init() before wandb.log()
Epoch 1:  43%|████▎     | 10/23 [00:53<00:34,  2.68s/it, loss=10.3960, lr=5.00e-06]Epoch 1:  43%|████▎     | 10/23 [00:54<00:34,  2.68s/it, loss=10.4956, lr=5.50e-06]Epoch 1:  48%|████▊     | 11/23 [00:54<00:29,  2.44s/it, loss=10.4956, lr=5.50e-06]Epoch 1:  48%|████▊     | 11/23 [01:41<00:29,  2.44s/it, loss=11.6715, lr=6.00e-06]Epoch 1:  52%|█████▏    | 12/23 [01:41<02:54, 15.89s/it, loss=11.6715, lr=6.00e-06]Epoch 1:  52%|█████▏    | 12/23 [02:20<02:54, 15.89s/it, loss=10.4550, lr=6.50e-06]Epoch 1:  57%|█████▋    | 13/23 [02:20<03:50, 23.00s/it, loss=10.4550, lr=6.50e-06]Epoch 1:  57%|█████▋    | 13/23 [03:29<03:50, 23.00s/it, loss=10.5852, lr=7.00e-06]Epoch 1:  61%|██████    | 14/23 [03:29<05:30, 36.68s/it, loss=10.5852, lr=7.00e-06]Epoch 1:  61%|██████    | 14/23 [03:31<05:30, 36.68s/it, loss=11.3363, lr=7.50e-06]Epoch 1:  65%|██████▌   | 15/23 [03:31<03:29, 26.19s/it, loss=11.3363, lr=7.50e-06]Epoch 1:  65%|██████▌   | 15/23 [03:33<03:29, 26.19s/it, loss=10.3015, lr=8.00e-06]Epoch 1:  70%|██████▉   | 16/23 [03:33<02:13, 19.09s/it, loss=10.3015, lr=8.00e-06]Epoch 1:  70%|██████▉   | 16/23 [03:35<02:13, 19.09s/it, loss=10.4089, lr=8.50e-06]Epoch 1:  74%|███████▍  | 17/23 [03:35<01:23, 13.92s/it, loss=10.4089, lr=8.50e-06]Epoch 1:  74%|███████▍  | 17/23 [03:36<01:23, 13.92s/it, loss=10.9681, lr=9.00e-06]Epoch 1:  78%|███████▊  | 18/23 [03:36<00:50, 10.16s/it, loss=10.9681, lr=9.00e-06]Epoch 1:  78%|███████▊  | 18/23 [03:39<00:50, 10.16s/it, loss=9.6357, lr=9.50e-06] Epoch 1:  83%|████████▎ | 19/23 [03:39<00:31,  7.85s/it, loss=9.6357, lr=9.50e-06]Epoch 1:  83%|████████▎ | 19/23 [03:42<00:31,  7.85s/it, loss=9.8275, lr=1.00e-05]Traceback (most recent call last):
  File "/work/courses/csnlp/Team3/slt/train_sign.py", line 372, in main
    wandb.log(log_dict)
  File "/work/courses/csnlp/Team3/envs/csnlp/lib/python3.10/site-packages/wandb/sdk/lib/preinit.py", line 36, in preinit_wrapper
    raise wandb.Error(f"You must call wandb.init() before {name}()")
wandb.errors.errors.Error: You must call wandb.init() before wandb.log()
Epoch 1:  87%|████████▋ | 20/23 [03:42<00:18,  6.33s/it, loss=9.8275, lr=1.00e-05]Epoch 1:  87%|████████▋ | 20/23 [03:44<00:18,  6.33s/it, loss=9.8159, lr=1.05e-05]Epoch 1:  91%|█████████▏| 21/23 [03:44<00:10,  5.25s/it, loss=9.8159, lr=1.05e-05]Epoch 1:  91%|█████████▏| 21/23 [03:46<00:10,  5.25s/it, loss=10.0465, lr=1.10e-05]Epoch 1:  96%|█████████▌| 22/23 [03:46<00:04,  4.15s/it, loss=10.0465, lr=1.10e-05]Epoch 1:  96%|█████████▌| 22/23 [03:47<00:04,  4.15s/it, loss=9.8908, lr=1.15e-05] Epoch 1: 100%|██████████| 23/23 [03:47<00:00,  3.19s/it, loss=9.8908, lr=1.15e-05]Epoch 1: 100%|██████████| 23/23 [03:47<00:00,  9.89s/it, loss=9.8908, lr=1.15e-05]
Epoch [1/20], Batch [1/23], Loss: 11.4508, LR: 5.00e-07
  Sample target: und der letzte sommertag sozusagen wird sich auch morgen mit wenig sommerlichen temperaturen vor all...
Training step failed: You must call wandb.init() before wandb.log()
Training step failed: You must call wandb.init() before wandb.log()
Epoch [1/20] finished. Average Training Loss: 10.6961
Epoch 2:   0%|          | 0/23 [00:00<?, ?it/s]Epoch 2:   0%|          | 0/23 [00:01<?, ?it/s, loss=9.5473, lr=1.20e-05]Epoch 2:   4%|▍         | 1/23 [00:01<00:34,  1.59s/it, loss=9.5473, lr=1.20e-05]Epoch 2:   4%|▍         | 1/23 [00:02<00:34,  1.59s/it, loss=9.8437, lr=1.25e-05]Epoch 2:   9%|▊         | 2/23 [00:02<00:28,  1.38s/it, loss=9.8437, lr=1.25e-05]Epoch 2:   9%|▊         | 2/23 [00:04<00:28,  1.38s/it, loss=9.6530, lr=1.30e-05]Epoch 2:  13%|█▎        | 3/23 [00:04<00:29,  1.49s/it, loss=9.6530, lr=1.30e-05]Epoch 2:  13%|█▎        | 3/23 [00:06<00:29,  1.49s/it, loss=9.6145, lr=1.35e-05]Epoch 2:  17%|█▋        | 4/23 [00:06<00:28,  1.53s/it, loss=9.6145, lr=1.35e-05]Epoch 2:  17%|█▋        | 4/23 [00:07<00:28,  1.53s/it, loss=9.8642, lr=1.40e-05]Epoch 2:  22%|██▏       | 5/23 [00:07<00:26,  1.47s/it, loss=9.8642, lr=1.40e-05]Epoch 2:  22%|██▏       | 5/23 [00:08<00:26,  1.47s/it, loss=9.2946, lr=1.45e-05]Epoch 2:  26%|██▌       | 6/23 [00:08<00:23,  1.38s/it, loss=9.2946, lr=1.45e-05]Epoch 2:  26%|██▌       | 6/23 [00:09<00:23,  1.38s/it, loss=9.3230, lr=1.50e-05]Traceback (most recent call last):
  File "/work/courses/csnlp/Team3/slt/train_sign.py", line 372, in main
    wandb.log(log_dict)
  File "/work/courses/csnlp/Team3/envs/csnlp/lib/python3.10/site-packages/wandb/sdk/lib/preinit.py", line 36, in preinit_wrapper
    raise wandb.Error(f"You must call wandb.init() before {name}()")
wandb.errors.errors.Error: You must call wandb.init() before wandb.log()
Epoch 2:  30%|███       | 7/23 [00:09<00:21,  1.33s/it, loss=9.3230, lr=1.50e-05]Epoch 2:  30%|███       | 7/23 [00:11<00:21,  1.33s/it, loss=9.2719, lr=1.55e-05]Epoch 2:  35%|███▍      | 8/23 [00:11<00:20,  1.38s/it, loss=9.2719, lr=1.55e-05]Epoch 2:  35%|███▍      | 8/23 [00:12<00:20,  1.38s/it, loss=8.9693, lr=1.60e-05]Epoch 2:  39%|███▉      | 9/23 [00:12<00:20,  1.45s/it, loss=8.9693, lr=1.60e-05]Epoch 2:  39%|███▉      | 9/23 [00:14<00:20,  1.45s/it, loss=9.5635, lr=1.65e-05]Epoch 2:  43%|████▎     | 10/23 [00:14<00:18,  1.40s/it, loss=9.5635, lr=1.65e-05]Epoch 2:  43%|████▎     | 10/23 [00:16<00:18,  1.40s/it, loss=9.3325, lr=1.70e-05]Epoch 2:  48%|████▊     | 11/23 [00:16<00:19,  1.62s/it, loss=9.3325, lr=1.70e-05]Epoch 2:  48%|████▊     | 11/23 [00:18<00:19,  1.62s/it, loss=9.0881, lr=1.75e-05]Epoch 2:  52%|█████▏    | 12/23 [00:18<00:18,  1.69s/it, loss=9.0881, lr=1.75e-05]Epoch 2:  52%|█████▏    | 12/23 [00:20<00:18,  1.69s/it, loss=8.6804, lr=1.80e-05]Epoch 2:  57%|█████▋    | 13/23 [00:20<00:19,  1.92s/it, loss=8.6804, lr=1.80e-05]Epoch 2:  57%|█████▋    | 13/23 [00:22<00:19,  1.92s/it, loss=8.7901, lr=1.85e-05]Epoch 2:  61%|██████    | 14/23 [00:22<00:16,  1.80s/it, loss=8.7901, lr=1.85e-05]Epoch 2:  61%|██████    | 14/23 [00:23<00:16,  1.80s/it, loss=9.1379, lr=1.90e-05]Epoch 2:  65%|██████▌   | 15/23 [00:23<00:13,  1.67s/it, loss=9.1379, lr=1.90e-05]Epoch 2:  65%|██████▌   | 15/23 [00:24<00:13,  1.67s/it, loss=8.8283, lr=1.95e-05]Epoch 2:  70%|██████▉   | 16/23 [00:24<00:11,  1.59s/it, loss=8.8283, lr=1.95e-05]Epoch 2:  70%|██████▉   | 16/23 [00:27<00:11,  1.59s/it, loss=8.3911, lr=2.00e-05]Traceback (most recent call last):
  File "/work/courses/csnlp/Team3/slt/train_sign.py", line 372, in main
    wandb.log(log_dict)
  File "/work/courses/csnlp/Team3/envs/csnlp/lib/python3.10/site-packages/wandb/sdk/lib/preinit.py", line 36, in preinit_wrapper
    raise wandb.Error(f"You must call wandb.init() before {name}()")
wandb.errors.errors.Error: You must call wandb.init() before wandb.log()
Epoch 2:  74%|███████▍  | 17/23 [00:27<00:11,  1.84s/it, loss=8.3911, lr=2.00e-05]Epoch 2:  74%|███████▍  | 17/23 [00:28<00:11,  1.84s/it, loss=8.4247, lr=2.05e-05]Epoch 2:  78%|███████▊  | 18/23 [00:28<00:08,  1.75s/it, loss=8.4247, lr=2.05e-05]Epoch 2:  78%|███████▊  | 18/23 [00:30<00:08,  1.75s/it, loss=8.8240, lr=2.10e-05]Epoch 2:  83%|████████▎ | 19/23 [00:30<00:06,  1.64s/it, loss=8.8240, lr=2.10e-05]Epoch 2:  83%|████████▎ | 19/23 [00:32<00:06,  1.64s/it, loss=8.2725, lr=2.15e-05]Epoch 2:  87%|████████▋ | 20/23 [00:32<00:05,  1.87s/it, loss=8.2725, lr=2.15e-05]Epoch 2:  87%|████████▋ | 20/23 [00:34<00:05,  1.87s/it, loss=8.3029, lr=2.20e-05]Epoch 2:  91%|█████████▏| 21/23 [00:34<00:03,  1.92s/it, loss=8.3029, lr=2.20e-05]Epoch 2:  91%|█████████▏| 21/23 [00:36<00:03,  1.92s/it, loss=8.4834, lr=2.25e-05]Epoch 2:  96%|█████████▌| 22/23 [00:36<00:01,  1.92s/it, loss=8.4834, lr=2.25e-05]Epoch 2:  96%|█████████▌| 22/23 [00:37<00:01,  1.92s/it, loss=7.7339, lr=2.30e-05]Epoch 2: 100%|██████████| 23/23 [00:37<00:00,  1.74s/it, loss=7.7339, lr=2.30e-05]Epoch 2: 100%|██████████| 23/23 [00:37<00:00,  1.65s/it, loss=7.7339, lr=2.30e-05]
Epoch [2/20], Batch [1/23], Loss: 9.5473, LR: 1.20e-05
  Sample target: sonst mal sonne mal wolken ab und an schauer vor allem im norden und westen am nachmittag einzelne g...
Training step failed: You must call wandb.init() before wandb.log()
Training step failed: You must call wandb.init() before wandb.log()
Epoch [2/20] finished. Average Training Loss: 9.0102
Starting evaluation for epoch 2...
Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]/work/courses/csnlp/Team3/envs/csnlp/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Evaluating:  33%|███▎      | 1/3 [00:05<00:10,  5.10s/it]Evaluating:  67%|██████▋   | 2/3 [00:10<00:05,  5.02s/it]Evaluating: 100%|██████████| 3/3 [00:13<00:00,  4.52s/it]Evaluating: 100%|██████████| 3/3 [00:13<00:00,  4.66s/it]

=== Evaluation Examples ===
Reference: alleine rostock warnemünde dort fielen in den drei sommermonaten mehr regen als sonst im ganzen jahr fällt
Generated: 7.8.9.10.11.12.13.14.16.17.18.19.20.21.22.23.24.25.

Reference: deutschland liegt innerhalb eines umfangreichen tiefs
Generated: 7.8.9.10.11.12.13.14.16.17.18.19.20.21.22.23.24.25.

Epoch [2/20] Validation Metrics: {'val_loss': 8.316625595092773, 'BLEU-1': 0.0, 'BLEU-2': 0.0, 'BLEU-3': 0.0, 'BLEU-4': 0.0, 'ROUGE-L': 0.0}
Traceback (most recent call last):
  File "/work/courses/csnlp/Team3/slt/train_sign.py", line 452, in <module>
    main()
  File "/work/courses/csnlp/Team3/slt/train_sign.py", line 395, in main
    wandb.log({
  File "/work/courses/csnlp/Team3/envs/csnlp/lib/python3.10/site-packages/wandb/sdk/lib/preinit.py", line 36, in preinit_wrapper
    raise wandb.Error(f"You must call wandb.init() before {name}()")
wandb.errors.errors.Error: You must call wandb.init() before wandb.log()
